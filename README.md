# Web Content Scraper

A powerful Node.js web scraper that extracts clean, readable content from websites while keeping everything nicely organized. Perfect for creating AI training datasets! ğŸ¤–

## âœ¨ Features

- ğŸŒ Smart recursive web crawling of internal links
- ğŸ“ Clean content extraction using Mozilla's Readability
- ğŸ§¹ Smart content processing and cleaning
- ğŸ—‚ï¸ Maintains original URL structure in saved files
- ğŸš« Excludes unwanted paths from scraping
- ğŸ”„ Handles relative and absolute URLs like a pro
- ğŸ¯ No duplicate page visits
- ğŸ“Š Generates JSONL output file for ML training
- ğŸ“Š AI-friendly clean text and csv output (perfect for LLM fine-tuning!)
- ğŸ“Š Rich metadata extraction
- ğŸ“ Combine results from multiple scrapers into a unified dataset

## ğŸ› ï¸ Prerequisites

- Node.js (v18 or higher)
- npm

## ğŸ“¦ Dependencies

- **axios** - HTTP requests master
- **jsdom** - DOM parsing wizard
- **@mozilla/readability** - Content extraction genius

## ğŸš€ Installation

```bash
npm i clean-web-scraper

# OR

git clone https://github.com/mlibre/Clean-Web-Scraper
cd Clean-Web-Scraper
npm install
```

## ğŸ’» Usage

```js
const WebScraper = require('clean-web-scraper');

const scraper = new WebScraper({
  baseURL: 'https://example.com/news',          // Required: The website base url to scrape
  startURL: 'https://example.com/blog',         // Optional: Custom starting URL
  excludeList: ['/admin', '/private'],          // Optional: Paths to exclude
  exactExcludeList: ['/specific-page'],         // Optional: Exact URLs to exclude
  scrapResultPath: './example.com/website',     // Required: Where to save the content
  jsonlOutputPath: './example.com/train.jsonl', // Optional: Custom JSONL output path
  textOutputPath: "./example.com/texts",        // Optional: Custom text output path
  csvOutputPath: "./example.com/train.csv",     // Optional: Custom CSV output path
  maxDepth: 3,                                  // Optional: Maximum depth for recursive crawling
  includeMetadata: false,                       // Optional: Include metadata in output files
  metadataFields: ['title', 'description']      // Optional: Specify metadata fields to include
});
scraper.start();

// Combine results from multiple scrapers
WebScraper.combineResults('./combined-dataset', [scraper1, scraper2]);
```

```bash
node example-usage.js
```

## ğŸ“¤ Output

Your AI-ready content is saved in a clean, structured format:

- ğŸ“ Base folder: ./folderPath/example.com/
- ğŸ“‘ Files preserve original URL paths
- ğŸ“ Pure text format, perfect for LLM training and fine-tuning
- ğŸ¤– No HTML, no mess - just clean, structured text ready for AI consumption
- ğŸ“Š JSONL output for ML training
- ğŸ“ˆ CSV output with clean text content

```bash
example.com/
â”œâ”€â”€ website/
â”‚   â”œâ”€â”€ page1.txt         # Clean text content
â”‚   â”œâ”€â”€ page1.json        # Full metadata
â”‚   â””â”€â”€ blog/
â”‚       â”œâ”€â”€ post1.txt
â”‚       â””â”€â”€ post1.json
â”œâ”€â”€ texts/                # Numbered text files
â”‚   â”œâ”€â”€ 1.txt
â”‚   â””â”€â”€ 2.txt
â”œâ”€â”€ texts_with_metadata/  # When includeMetadata is true
â”‚   â”œâ”€â”€ 1.txt
â”‚   â””â”€â”€ 2.txt
â”œâ”€â”€ train.jsonl           # Combined content
â”œâ”€â”€ train_with_metadata.jsonl  # When includeMetadata is true
â”œâ”€â”€ train.csv             # Clean text in CSV format
â””â”€â”€ train_with_metadata.csv    # When includeMetadata is true
```

## ğŸ¤– AI/LLM Training Ready

The output is specifically formatted for AI training purposes:

- Clean, processed text without HTML markup
- Multiple formats (JSONL, CSV, text files)
- Structured content perfect for fine-tuning LLMs
- Ready to use in your ML pipelines

## Standing with Palestine ğŸ‡µğŸ‡¸

This project supports Palestinian rights and stands in solidarity with Palestine. We believe in the importance of documenting and preserving Palestinian narratives, history, and struggles for justice and liberation.

Free Palestine ğŸ‡µğŸ‡¸
