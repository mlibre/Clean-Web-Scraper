# Web Content Scraper

A powerful Node.js web scraper that extracts clean, readable content from websites while keeping everything nicely organized. Perfect for creating AI training datasets! ğŸ¤–

## âœ¨ Features

- ğŸŒ Smart recursive web crawling of internal links
- ğŸ“ Clean content extraction using Mozilla's Readability
- ğŸ§¹ Smart content processing and cleaning
- ğŸ—‚ï¸ Maintains original URL structure in saved files
- ğŸš« Excludes unwanted paths from scraping
- ğŸ”„ Handles relative and absolute URLs like a pro
- ğŸ¯ No duplicate page visits
- ğŸ¤– AI-friendly output formats (JSONL, CSV, clean text)
- ğŸ“Š Rich metadata extraction
- ğŸ“ Combine results from multiple scrapers into a unified dataset
- ğŸ¯ Turn any website into an AI training dataset

## ğŸ› ï¸ Prerequisites

- Node.js (v18 or higher)
- npm

## ğŸ“¦ Dependencies

- **axios** - HTTP requests master
- **jsdom** - DOM parsing wizard
- **@mozilla/readability** - Content extraction genius

## ğŸš€ Installation

```bash
npm i clean-web-scraper

# OR

git clone https://github.com/mlibre/Clean-Web-Scraper
cd Clean-Web-Scraper
npm install
```

## ğŸ’» Usage

```js
const WebScraper = require('clean-web-scraper');

const scraper = new WebScraper({
  baseURL: 'https://example.com/news',          // Required: The website base url to scrape
  startURL: 'https://example.com/blog',         // Optional: Custom starting URL
  excludeList: ['/admin', '/private'],          // Optional: Paths to exclude
  exactExcludeList: ['/specific-page'],         // Optional: Exact URLs to exclude
  scrapResultPath: './example.com/website',     // Required: Where to save the content
  jsonlOutputPath: './example.com/train.jsonl', // Optional: Custom JSONL output path
  textOutputPath: "./example.com/texts",        // Optional: Custom text output path
  csvOutputPath: "./example.com/train.csv",     // Optional: Custom CSV output path
  maxDepth: 3,                                  // Optional: Maximum depth for recursive crawling
  includeMetadata: false,                       // Optional: Include metadata in output files
  metadataFields: ['title', 'description']      // Optional: Specify metadata fields to include
});
await scraper.start();
```

```bash
node example-usage.js
```

## ğŸ’» Advanced Usage: Multi-Site Scraping

```js
const WebScraper = require('clean-web-scraper');

// Scrape documentation website
const docsScraper = new WebScraper({
  baseURL: 'https://docs.example.com',
  scrapResultPath: './datasets/docs'
});

// Scrape blog website
const blogScraper = new WebScraper({
  baseURL: 'https://blog.example.com',
  scrapResultPath: './datasets/blog'
});

// Start scraping both sites
await docsScraper.start();
await blogScraper.start();

// Combine all scraped content into a single dataset
await WebScraper.combineResults('./combined-dataset', [docsScraper, blogScraper]);
```

## ğŸ“¤ Output

Your AI-ready content is saved in a clean, structured format:

- ğŸ“ Base folder: ./folderPath/example.com/
- ğŸ“‘ Files preserve original URL paths
- ğŸ“ Pure text format, perfect for LLM training and fine-tuning
- ğŸ¤– No HTML, no mess - just clean, structured text ready for AI consumption
- ğŸ“Š JSONL output for ML training
- ğŸ“ˆ CSV output with clean text content

```bash
example.com/
â”œâ”€â”€ website/
â”‚   â”œâ”€â”€ page1.txt         # Clean text content
â”‚   â”œâ”€â”€ page1.json        # Full metadata
â”‚   â””â”€â”€ blog/
â”‚       â”œâ”€â”€ post1.txt
â”‚       â””â”€â”€ post1.json
â”œâ”€â”€ texts/                # Numbered text files
â”‚   â”œâ”€â”€ 1.txt
â”‚   â””â”€â”€ 2.txt
â”œâ”€â”€ texts_with_metadata/  # When includeMetadata is true
â”‚   â”œâ”€â”€ 1.txt
â”‚   â””â”€â”€ 2.txt
â”œâ”€â”€ train.jsonl           # Combined content
â”œâ”€â”€ train_with_metadata.jsonl  # When includeMetadata is true
â”œâ”€â”€ train.csv             # Clean text in CSV format
â””â”€â”€ train_with_metadata.csv    # When includeMetadata is true
```

## ğŸ¤– AI/LLM Training Ready

The output is specifically formatted for AI training and fine-tuning purposes:

- Clean, processed text without HTML markup
- Multiple formats (JSONL, CSV, text files)
- Structured content perfect for fine-tuning LLMs
- Ready to use in your ML pipelines

## Standing with Palestine ğŸ‡µğŸ‡¸

This project supports Palestinian rights and stands in solidarity with Palestine. We believe in the importance of documenting and preserving Palestinian narratives, history, and struggles for justice and liberation.

Free Palestine ğŸ‡µğŸ‡¸
